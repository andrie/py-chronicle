# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/core.ipynb.

# %% auto 0
__all__ = ['read_chronicle', 'read_chronicle_metrics', 'read_chronicle_logs', 'scan_chronicle', 'scan_chronicle_metrics',
           'scan_chronicle_logs', 'ChronicleMetrics', 'ChronicleLogs']

# %% ../nbs/core.ipynb 3
import polars as pl
import pyarrow.parquet as pq
import pyarrow.dataset as ds
from s3fs import S3FileSystem
import pandas as pd
from fastcore.basics import patch
import re

# %% ../nbs/core.ipynb 6
def read_chronicle(
        path: str, # Path to dataset,
        type: str = "", # must be `metrics` or `logs`
        date:str = "", # date in format `YYYY/MM/DD` 
        version: str = "v1" # currently must be `v1`
    ) -> pl.DataFrame:
    "Read a chronicle parquet file into a polars dataframe."
    path = f"{path}/{version}/{type}/{date}"
    return pl.from_arrow(pq.read_table(path))



# %% ../nbs/core.ipynb 7
def read_chronicle_metrics(
        path: str, # Path to dataset,
        date:str = "", # date in format `YYYY/MM/DD` 
        version: str = "v1" # currently must be `v1`
) -> pl.DataFrame:
    "Read a chronicle metrics parquet file into a polars dataframe."
    return read_chronicle(path, "metrics", date, version = version) 

def read_chronicle_logs(
        path: str, # Path to dataset,
        date:str = "", # date in format `YYYY/MM/DD` 
        version: str = "v1" # currently must be `v1`
) -> pl.DataFrame:
    "Read a chronicle logs parquet file into a polars dataframe."
    return read_chronicle(path, "logs", date, version) 


# %% ../nbs/core.ipynb 11
def scan_chronicle(
        path: str, # Path to dataset,
        type: str = "", # must be `metrics` or `logs`
        date:str = None, # date in format `YYYY/MM/DD` 
        filename: str = None, # name of parquet file. If empty, will be inferred.
        version: str = "v1" # currently must be `v1`
    ) -> pl.LazyFrame:
    "Read a chronicle parquet file into a polars LazyFrame."
    if date == None:
        date = "*/*/*"
    else:
        date = re.sub("-", "/", date)
        dateh = re.sub("/", "-", date)
    if filename == None:
        # filename = f"{type}-{dateh}.parquet"
        filename = "*.parquet"
    path = f"{path}/{version}/{type}/{date}/{filename}"
    # return path
    return pl.scan_parquet(path)



# %% ../nbs/core.ipynb 13
def scan_chronicle_metrics(
        path: str, # Path to dataset,
        date:str = None, # date in format `YYYY/MM/DD` 
        version: str = "v1" # currently must be `v1`
) -> pl.DataFrame:
    "Read a chronicle metrics parquet file into a polars dataframe."
    return scan_chronicle(path, "metrics", date, version = version) 

def scan_chronicle_logs(
        path: None, # Path to dataset,
        date:str = None, # date in format `YYYY/MM/DD` 
        version: str = "v1" # currently must be `v1`
) -> pl.DataFrame:
    "Read a chronicle logs parquet file into a polars dataframe."
    return scan_chronicle(path, "logs", date, version = version) 


# %% ../nbs/core.ipynb 17
# @pl.api.register_dataframe_namespace("metrics")
# class ChronicleMetrics:
#     def __init__(self, 
#                  df: pl.DataFrame # A `polars` DataFrame
#                  ) -> pl.DataFrame:
#         "Initialise a chronicle metrics class"
#         self._df = df

@pl.api.register_lazyframe_namespace("metrics")
class ChronicleMetrics:
    def __init__(self, 
                 ldf: pl.LazyFrame # A `polars` DataFrame
                 ) -> pl.LazyFrame:
        "Initialise a chronicle metrics class"
        self._ldf = ldf



# %% ../nbs/core.ipynb 19
@patch
def describe(self: ChronicleMetrics) -> pl.DataFrame:
    "Reads metrics dataframe and returns a pandas dataframe with summary of service, name and description of all metrics"
    return (
        self._ldf
        .groupby("service", "name")
        .agg(
            pl.col("description").unique(),
            pl.col("value_column").unique(),
        )
        .with_columns(
            pl.col("description").arr.join(", "),
            pl.col("value_column").arr.join("")
        )
        .sort("service", "name")
        .collect()
        .to_pandas()
    )



# %% ../nbs/core.ipynb 23
@patch
def filter(self: ChronicleMetrics, 
        name:str, # name of metric to extract
        service:str = None, # service to extract metric from
        alias:str = None # alias to use for new column
    ) -> pd.DataFrame:
    "Extract a single metric from a metrics dataframe"
    if alias == None:
        alias = name
    
    df = (
        self._ldf
        .filter(
            pl.col("name") == name
        )
    )
    
    if service != None:
        df = df.filter(pl.col("service") == service) 
    
    return (
        df
        .sort(pl.col("host"), pl.col("timestamp"))
        .select([
            "host",
            pl.col("timestamp"),
            pl.col("value_float").alias(alias)
        ])
        .collect()
        .to_pandas()
    )


# %% ../nbs/core.ipynb 26
@pl.api.register_lazyframe_namespace("logs")
class ChronicleLogs:
    def __init__(self, 
                 df: pl.DataFrame # A polars data frame
                 ) -> pl.DataFrame:
        "Initialise a chronicle logs DataFrame"
        self._ldf = df

# %% ../nbs/core.ipynb 29
@patch
def filter_type(self: ChronicleLogs,
                value: str # Value to extract 
    ) -> pd.DataFrame:
    "Extract all logs where type == value"
    return (
        self._ldf
        .with_columns([
            (pl.col("body").str.json_path_match(f"$.{value}").alias(f".{value}")),
            (pl.col("body").str.json_path_match("$.type").alias(".type"))
        ])
        .filter(pl.col(f".{value}").is_not_null())
        .select(["service", "host", "timestamp", f".{value}", ".type", "body"])
        .sort("service", "host", f".{value}", "timestamp")
        .collect()
    )

# %% ../nbs/core.ipynb 32
@patch
def unique_connect_actions(self: ChronicleLogs,
    ) -> pd.DataFrame:
    "Extract a sample of unique connect actions"
    return (
        self._ldf
        .filter(pl.col("service") == "connect")
        .with_columns([
                pl.col("body").str.json_path_match("$.msg").alias("message"),
                pl.col("body").str.json_path_match("$.actor_description").alias("username"),
                pl.col("body").str.json_path_match("$.action").alias("action"),
        ])
        .unique("action")
        .select("service", "action", "attributes", "body")
        .collect()
        .to_pandas()
    )


# %% ../nbs/core.ipynb 35
@patch
def connect_logins(
    self: ChronicleLogs,
    ) -> pl.DataFrame:
    "Extract Connect login logs"
    return (
        self._ldf
        .with_columns([
            pl.col("body").str.json_path_match("$.type").alias("type"),
            pl.col("body").str.json_path_match("$.action").alias("action"),
            pl.col("body").str.json_path_match("$.actor_description").alias("username"),
        ])
        .filter(
            (pl.col("service") == "connect") &
            (pl.col("type") == "audit") &
            (pl.col("action") == "user_login")
        )
        .select("host", "timestamp", "username", "action", "type")
        .collect()
    )


# %% ../nbs/core.ipynb 38
@patch
def extract_connect_audit_logs(
    self: ChronicleLogs,
    type: str,
    ) -> pl.DataFrame:
    "Extract Connect audit logs"
    return (
        self._ldf
        .with_columns([
            pl.col("body").str.json_path_match("$.type").alias("type"),
            pl.col("body").str.json_path_match("$.action").alias("action"),
            pl.col("body").str.json_path_match("$.actor_description").alias("username"),
            pl.col("body").str.json_path_match("$.actor_guid").alias("guid"),
            pl.col("body").str.json_path_match("$.msg").alias("msg"),
        ])
        .filter(
            (pl.col("service") == "connect") &
            (pl.col("action") == type)
        )
        .select("host", "timestamp", "username", "action", "type", "guid", "msg", "attributes")
        .collect()
    )




# %% ../nbs/core.ipynb 41
@patch
def unique_workbench_types(self: ChronicleLogs,
    ) -> pd.DataFrame:
    "Extract a sample of unique workbench types"
    return (
        self._ldf
        .filter(pl.col("service") == "workbench")
        .with_columns([
                pl.col("body").str.json_path_match("$.type").alias("type"),
                pl.col("body").str.json_path_match("$.username").alias("username"),
        ])
        .unique("type")
        .select("service", "type", "attributes", "body")
        .collect()
        .to_pandas()
    )


# %% ../nbs/core.ipynb 44
@patch
def workbench_logins(
    self: ChronicleLogs,
    ) -> pl.DataFrame:
    "Extract Workbench login logs"
    return (
        self._ldf
        .with_columns([
            pl.col("body").str.json_path_match("$.type").alias("type"),
            pl.col("body").str.json_path_match("$.action").alias("action"),
            pl.col("body").str.json_path_match("$.username").alias("username"),
        ])
        .filter(
            (pl.col("service") == "workbench") &
            (pl.col("type") == "auth_login")
        )
        .select("host", "timestamp", "username", "action", "type")
        .collect()
    )


# %% ../nbs/core.ipynb 47
@patch
def extract_workbench_audit_logs(
    self: ChronicleLogs,
    type: str,
    ) -> pl.DataFrame:
    "Extract Workbench login logs"
    return (
        self._ldf
        .with_columns([
            pl.col("body").str.json_path_match("$.type").alias("type"),
            pl.col("body").str.json_path_match("$.action").alias("action"),
            pl.col("body").str.json_path_match("$.username").alias("username"),
        ])
        .filter(
            (pl.col("service") == "workbench") &
            (pl.col("type") == type)
        )
        .select("host", "timestamp", "username", "action", "type", "attributes")
        .collect()
    )



# %% ../nbs/core.ipynb 49
@patch
def extract_workbench_audit_cols(
    self: ChronicleLogs,
    type: str,
    ) -> pl.DataFrame:
    "Extract Workbench audit columns"
    return (
        self._ldf
        .with_columns([
            pl.col("body").str.json_path_match("$.type").alias("type"),
            pl.col("body").str.json_path_match("$.action").alias("action"),
            pl.col("body").str.json_path_match("$.username").alias("username"),
        ])
        .filter(
            (pl.col("service") == "workbench") &
            (pl.col("type") == type)
        )
        .select("host", "timestamp", "username", "action", "type", "attributes")
        # .head(1)
        # .explode("attributes").unnest("attributes") #.drop("value")
        # .with_columns([
        #     # pl.col("attributes").arr.to_struct().apply(lambda x: x[1])
        #     pl.col("attributes").apply(lambda x: x.sort())
        #     # pl.col("attributes").arr.to_struct().to_dict()
        #     # pl.col("attributes").to_dict()
        # ])
        # .select("attributes")
        .collect()
        # .to_struct("attributes")
        # .explode("attributes")
        # .unnest()
        # .to_dicts()
        # .to_pandas()
    )


