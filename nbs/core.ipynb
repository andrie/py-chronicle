{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Scan and query chronicle parquet files.\n",
    "\n",
    "This package uses [polars](https://pola-rs.github.io/polars/py-polars/html/index.html) to read and query chronicle parquet files, and expose a simple API to query the resulting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import polars as pl\n",
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow.dataset as ds\n",
    "# from s3fs import S3FileSystem\n",
    "import pandas as pd\n",
    "from fastcore.basics import patch\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan chronicle parquet files\n",
    "\n",
    "Chronicle collects and stores logs and metrics in a series of parquet files.\n",
    "\n",
    "Use `scan_chronicle_logs()` to read logs and `scan_chronicle_logs()` to read metrics, by specifying the path to the parquet set you need.\n",
    "\n",
    "The file tree looks like this, with `logs` and `metrics` in separate folders inside `v1`.\n",
    "\n",
    "``` bash\n",
    ".\n",
    "└── v1/\n",
    "    ├── logs/\n",
    "    └── metrics/\n",
    "```\n",
    "\n",
    "Inside both `logs` and `metrics` the data is stored by date, separated by year, month and day.\n",
    "\n",
    "``` bash\n",
    ".\n",
    "└── v1/\n",
    "    ├── logs/\n",
    "    │   └── 2023/\n",
    "    │       ├── 02/\n",
    "    │       │   ├── 01\n",
    "    │       │   ├── 02\n",
    "    │       │   ├── 03\n",
    "    │       │   ├── 04\n",
    "    │       │   ├── 05\n",
    "    │       │   └── ...\n",
    "    │       ├── 03\n",
    "    │       ├── 04\n",
    "    │       └── ...\n",
    "    └── metrics/\n",
    "        └── 2023/\n",
    "            ├── 02/\n",
    "            │   ├── 01\n",
    "            │   ├── 02\n",
    "            │   ├── 03\n",
    "            │   ├── 04\n",
    "            │   ├── 05\n",
    "            │   └── ...\n",
    "            ├── 03\n",
    "            ├── 04\n",
    "            └── ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the scan interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scan_chronicle(\n",
    "        path: str, # Path to dataset,\n",
    "        type: str = \"\", # must be `metrics` or `logs`\n",
    "        date:str = None, # date in format `YYYY/MM/DD` \n",
    "        filename: str = None, # name of parquet file. If empty, will be inferred.\n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    "    ) -> pl.LazyFrame:\n",
    "    \"Read a chronicle parquet file into a polars LazyFrame.\"\n",
    "    if date == None:\n",
    "        date = \"*/*/*\"\n",
    "    else:\n",
    "        date = re.sub(\"-\", \"/\", date)\n",
    "        dateh = re.sub(\"/\", \"-\", date)\n",
    "    if filename == None:\n",
    "        # filename = f\"{type}-{dateh}.parquet\"\n",
    "        filename = \"*.parquet\"\n",
    "    path = f\"{path}/{version}/{type}/{date}/{filename}\"\n",
    "    # return path\n",
    "    return pl.scan_parquet(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scan_chronicle(\"./data\", \"metrics\", \"2023/04/03\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scan_chronicle_metrics(\n",
    "        path: str, # Path to dataset,\n",
    "        date:str = None, # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    ") -> pl.DataFrame:\n",
    "    \"Read a chronicle metrics parquet file into a polars dataframe.\"\n",
    "    return scan_chronicle(path, \"metrics\", date, version = version) \n",
    "\n",
    "def scan_chronicle_logs(\n",
    "        path: None, # Path to dataset,\n",
    "        date:str = None, # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    ") -> pl.DataFrame:\n",
    "    \"Read a chronicle logs parquet file into a polars dataframe.\"\n",
    "    return scan_chronicle(path, \"logs\", date, version = version) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = scan_chronicle_metrics(\"./data\", \"2023/04/03\")\n",
    "assert type(z) == pl.LazyFrame\n",
    "assert z.collect().columns == [\n",
    "    'service',\n",
    "    'host',\n",
    "    'os',\n",
    "    'attributes',\n",
    "    'name',\n",
    "    'description',\n",
    "    'unit',\n",
    "    'type',\n",
    "    'timestamp',\n",
    "    'value_float',\n",
    "    'value_int',\n",
    "    'value_uint',\n",
    "    'value_column'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = scan_chronicle_logs(\"./data\", \"2023/04/03\")\n",
    "assert type(z) == pl.LazyFrame\n",
    "assert z.collect().columns == [\n",
    "    'service', \n",
    "    'host', \n",
    "    'os', \n",
    "    'attributes', \n",
    "    'body', \n",
    "    'timestamp'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@pl.api.register_lazyframe_namespace(\"metrics\")\n",
    "class ChronicleMetrics:\n",
    "    def __init__(self, \n",
    "                 ldf: pl.LazyFrame # A `polars` DataFrame\n",
    "                 ) -> pl.LazyFrame:\n",
    "        \"Initialise a chronicle metrics class\"\n",
    "        self._ldf = ldf\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.metrics.describe()` to get a DataFrame of the unique metrics in the metrics data, containing the `service`, `name` and `description` of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def describe(self: ChronicleMetrics) -> pd.DataFrame:\n",
    "    \"Reads metrics dataframe and returns a pandas dataframe with summary of service, name and description of all metrics\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .groupby(\"service\", \"name\")\n",
    "        .agg(\n",
    "            pl.col(\"description\").unique(),\n",
    "            pl.col(\"value_column\").unique(),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"description\").list.join(\", \"),\n",
    "            pl.col(\"value_column\").list.join(\"\")\n",
    "        )\n",
    "        .sort(\"service\", \"name\")\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.describe()\n",
    "assert list(m) == ['service', 'name', 'description', 'value_column']\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.metrics.filter()` to filter the DataFrame on the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter(self: ChronicleMetrics, \n",
    "        name:str, # name of metric to extract\n",
    "        service:str = None, # service to extract metric from\n",
    "        alias:str = None # alias to use for new column\n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract a single metric from a metrics dataframe\"\n",
    "    if alias == None:\n",
    "        alias = name\n",
    "    \n",
    "    df = (\n",
    "        self._ldf\n",
    "        .filter(\n",
    "            pl.col(\"name\") == name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if service != None:\n",
    "        df = df.filter(pl.col(\"service\") == service) \n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        .sort(pl.col(\"host\"), pl.col(\"timestamp\"))\n",
    "        .select([\n",
    "            \"host\",\n",
    "            pl.col(\"timestamp\"),\n",
    "            pl.col(\"value_float\").alias(alias)\n",
    "        ])\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.filter(\"rsconnect_system_memory_used\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'rsconnect_system_memory_used']\n",
    "\n",
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.filter(\"rsconnect_system_memory_used\", alias=\"memory\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'memory']\n",
    "\n",
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.filter(\"rsconnect_system_memory_used\", service = \"connect-metrics\", alias = \"memory\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'memory']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@pl.api.register_lazyframe_namespace(\"logs\")\n",
    "class ChronicleLogs:\n",
    "    def __init__(self, \n",
    "                 df: pl.DataFrame # A polars data frame\n",
    "                 ) -> pl.DataFrame:\n",
    "        \"Initialise a chronicle logs DataFrame\"\n",
    "        self._ldf = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter logs on type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `logs/filter_type()` to filter logs on the `type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scan_chronicle_logs(\"./data\").head(1).explode(\"attributes\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter_type(self: ChronicleLogs,\n",
    "                value: str # Value to extract \n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract all logs where type == value\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            # (pl.col(\"body\").str.json_path_match(f\"$.{value}\").alias(f\".{value}\")),\n",
    "            (pl.col(\"body\").str.json_path_match(\"$.type\").alias(\".type\"))\n",
    "        ])\n",
    "        .filter(pl.col(\".type\") == f\"{value}\")\n",
    "        # .select([\"service\", \"host\", \"timestamp\", f\".{value}\", \".type\", \"body\"])\n",
    "        # .sort(\"service\", \"host\", f\".{value}\", \"timestamp\")\n",
    "        .collect()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_chronicle_logs(\"./data\").logs.filter_type(\"auth_login\")\n",
    "logs = scan_chronicle_logs(\"./data\").logs.filter_type(\"username\")\n",
    "assert type(logs) == pl.DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Connect actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@patch\n",
    "def unique_connect_actions(self: ChronicleLogs,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract a sample of unique connect actions\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .filter(pl.col(\"service\") == \"connect\")\n",
    "        .with_columns([\n",
    "                pl.col(\"body\").str.json_path_match(\"$.msg\").alias(\"message\"),\n",
    "                pl.col(\"body\").str.json_path_match(\"$.actor_description\").alias(\"username\"),\n",
    "                pl.col(\"body\").str.json_path_match(\"$.action\").alias(\"action\"),\n",
    "        ])\n",
    "        .unique(\"action\")\n",
    "        .select(\"service\", \"action\", \"attributes\", \"body\")\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_chronicle_logs(\"./data\").logs.unique_connect_actions()\n",
    "# assert type(logs) == pl.DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@patch\n",
    "def connect_logins(\n",
    "    self: ChronicleLogs,\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Extract Connect login logs\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            pl.col(\"body\").str.json_path_match(\"$.type\").alias(\"type\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.action\").alias(\"action\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.actor_description\").alias(\"username\"),\n",
    "        ])\n",
    "        .filter(\n",
    "            (pl.col(\"service\") == \"connect\") &\n",
    "            (pl.col(\"type\") == \"audit\") &\n",
    "            (pl.col(\"action\") == \"user_login\")\n",
    "        )\n",
    "        .select(\"host\", \"timestamp\", \"username\", \"action\", \"type\")\n",
    "        .collect()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data\"\n",
    "scan_chronicle_logs(path).logs.connect_logins()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract connect audit logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def extract_connect_audit_logs(\n",
    "    self: ChronicleLogs,\n",
    "    type: str,\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Extract Connect audit logs\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            pl.col(\"body\").str.json_path_match(\"$.type\").alias(\"type\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.action\").alias(\"action\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.actor_description\").alias(\"username\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.actor_guid\").alias(\"guid\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.msg\").alias(\"msg\"),\n",
    "        ])\n",
    "        .filter(\n",
    "            (pl.col(\"service\") == \"connect\") &\n",
    "            (pl.col(\"action\") == type)\n",
    "        )\n",
    "        .select(\"host\", \"timestamp\", \"username\", \"action\", \"type\", \"guid\", \"msg\", \"attributes\")\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data\"\n",
    "scan_chronicle_logs(path).logs.extract_connect_audit_logs(\"user_login\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique workbench types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@patch\n",
    "def unique_workbench_types(self: ChronicleLogs,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract a sample of unique workbench types\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .filter(pl.col(\"service\") == \"workbench\")\n",
    "        .with_columns([\n",
    "                pl.col(\"body\").str.json_path_match(\"$.type\").alias(\"type\"),\n",
    "                pl.col(\"body\").str.json_path_match(\"$.username\").alias(\"username\"),\n",
    "        ])\n",
    "        .unique(\"type\")\n",
    "        .select(\"service\", \"type\", \"attributes\", \"body\")\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_chronicle_logs(\"./data\").logs.unique_workbench_types()\n",
    "# assert type(logs) == pl.DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workbench logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def workbench_logins(\n",
    "    self: ChronicleLogs,\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Extract Workbench login logs\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            pl.col(\"body\").str.json_path_match(\"$.type\").alias(\"type\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.action\").alias(\"action\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.username\").alias(\"username\"),\n",
    "        ])\n",
    "        .filter(\n",
    "            (pl.col(\"service\") == \"workbench\") &\n",
    "            (pl.col(\"type\") == \"auth_login\")\n",
    "        )\n",
    "        .select(\"host\", \"timestamp\", \"username\", \"action\", \"type\")\n",
    "        .collect()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data\"\n",
    "scan_chronicle_logs(path).logs.workbench_logins()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract workbench audit logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def extract_workbench_audit_logs(\n",
    "    self: ChronicleLogs,\n",
    "    type: str,\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Extract Workbench login logs\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            pl.col(\"body\").str.json_path_match(\"$.type\").alias(\"type\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.action\").alias(\"action\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.username\").alias(\"username\"),\n",
    "        ])\n",
    "        .filter(\n",
    "            (pl.col(\"service\") == \"workbench\") &\n",
    "            (pl.col(\"type\") == type)\n",
    "        )\n",
    "        .select(\"host\", \"timestamp\", \"username\", \"action\", \"type\", \"attributes\")\n",
    "        .collect()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data\"\n",
    "scan_chronicle_logs(path).logs.extract_workbench_audit_logs(\"session_start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def extract_workbench_audit_cols(\n",
    "    self: ChronicleLogs,\n",
    "    type: str,\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Extract Workbench audit columns\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            pl.col(\"body\").str.json_path_match(\"$.type\").alias(\"type\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.action\").alias(\"action\"),\n",
    "            pl.col(\"body\").str.json_path_match(\"$.username\").alias(\"username\"),\n",
    "        ])\n",
    "        .filter(\n",
    "            (pl.col(\"service\") == \"workbench\") &\n",
    "            (pl.col(\"type\") == type)\n",
    "        )\n",
    "        .select(\"host\", \"timestamp\", \"username\", \"action\", \"type\", \"attributes\")\n",
    "        # .head(1)\n",
    "        # .explode(\"attributes\").unnest(\"attributes\") #.drop(\"value\")\n",
    "        # .with_columns([\n",
    "        #     # pl.col(\"attributes\").arr.to_struct().apply(lambda x: x[1])\n",
    "        #     pl.col(\"attributes\").apply(lambda x: x.sort())\n",
    "        #     # pl.col(\"attributes\").arr.to_struct().to_dict()\n",
    "        #     # pl.col(\"attributes\").to_dict()\n",
    "        # ])\n",
    "        # .select(\"attributes\")\n",
    "        .collect()\n",
    "        # .to_struct(\"attributes\")\n",
    "        # .explode(\"attributes\")\n",
    "        # .unnest()\n",
    "        # .to_dicts()\n",
    "        # .to_pandas()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_chronicle_logs(\"./data\").logs.extract_workbench_audit_cols(\"session_quit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
