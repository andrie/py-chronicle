{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Read and query chronicle parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "from s3fs import S3FileSystem\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from fastcore.basics import patch\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read chronicle parquet files\n",
    "\n",
    "Chronicle collects and stores logs and metrics in a series of parquet files.\n",
    "\n",
    "Use `read_chronicle()` to read either logs or metrics, by specifying the path to the parquet set you need.\n",
    "\n",
    "The file tree looks like this, with `logs` and `metrics` in separate folders inside `v1`.\n",
    "\n",
    "``` bash\n",
    ".\n",
    "└── v1/\n",
    "    ├── logs/\n",
    "    └── metrics/\n",
    "```\n",
    "\n",
    "Inside both `logs` and `metrics` the data is stored by date, separated by year, month and day.\n",
    "\n",
    "``` bash\n",
    ".\n",
    "└── v1/\n",
    "    ├── logs/\n",
    "    │   └── 2023/\n",
    "    │       ├── 02/\n",
    "    │       │   ├── 01\n",
    "    │       │   ├── 02\n",
    "    │       │   ├── 03\n",
    "    │       │   ├── 04\n",
    "    │       │   ├── 05\n",
    "    │       │   └── ...\n",
    "    │       ├── 03\n",
    "    │       ├── 04\n",
    "    │       └── ...\n",
    "    └── metrics/\n",
    "        └── 2023/\n",
    "            ├── 02/\n",
    "            │   ├── 01\n",
    "            │   ├── 02\n",
    "            │   ├── 03\n",
    "            │   ├── 04\n",
    "            │   ├── 05\n",
    "            │   └── ...\n",
    "            ├── 03\n",
    "            ├── 04\n",
    "            └── ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the read interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def read_chronicle(\n",
    "        path: str, # Path to dataset,\n",
    "        type: str = \"\", # must be `metrics` or `logs`\n",
    "        date:str = \"\", # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Read a chronicle parquet file into a polars dataframe.\"\n",
    "    path = f\"{path}/{version}/{type}/{date}\"\n",
    "    return pl.from_arrow(pq.read_table(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_chronicle_metrics(\n",
    "        path: str, # Path to dataset,\n",
    "        date:str = \"\", # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    ") -> pl.DataFrame:\n",
    "    \"Read a chronicle metrics parquet file into a polars dataframe.\"\n",
    "    return read_chronicle(path, \"metrics\", date, version = version) \n",
    "\n",
    "def read_chronicle_logs(\n",
    "        path: str, # Path to dataset,\n",
    "        date:str = \"\", # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    ") -> pl.DataFrame:\n",
    "    \"Read a chronicle logs parquet file into a polars dataframe.\"\n",
    "    return read_chronicle(path, \"logs\", date, version) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = read_chronicle_metrics(\"./data\", \"2023/04/03\")\n",
    "assert type(z) == pl.DataFrame\n",
    "assert z.columns == [\n",
    "    'service',\n",
    "    'host',\n",
    "    'os',\n",
    "    'attributes',\n",
    "    'name',\n",
    "    'description',\n",
    "    'unit',\n",
    "    'type',\n",
    "    'timestamp',\n",
    "    'value_float',\n",
    "    'value_int',\n",
    "    'value_uint',\n",
    "    'value_column'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = read_chronicle_logs(\"./data\", \"2023/04/03\")\n",
    "assert type(z) == pl.dataframe.frame.DataFrame\n",
    "assert z.columns == [\n",
    "    'service', \n",
    "    'host', \n",
    "    'os', \n",
    "    'attributes', \n",
    "    'body', \n",
    "    'timestamp'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the scan interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scan_chronicle(\n",
    "        path: str, # Path to dataset,\n",
    "        type: str = \"\", # must be `metrics` or `logs`\n",
    "        date:str = \"\", # date in format `YYYY/MM/DD` \n",
    "        filename: str = None, # name of parquet file. If empty, will be inferred.\n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    "    ) -> pl.LazyFrame:\n",
    "    \"Read a chronicle parquet file into a polars LazyFrame.\"\n",
    "    date = re.sub(\"-\", \"/\", date)\n",
    "    dateh = re.sub(\"/\", \"-\", date)\n",
    "    if filename == None:\n",
    "        filename = f\"{type}-{dateh}.parquet\"\n",
    "    path = f\"{path}/{version}/{type}/{date}/{filename}\"\n",
    "    # return path\n",
    "    return pl.scan_parquet(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scan_chronicle(\"./data\", \"metrics\", \"2023/04/03\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scan_chronicle_metrics(\n",
    "        path: str, # Path to dataset,\n",
    "        date:str = \"\", # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    ") -> pl.DataFrame:\n",
    "    \"Read a chronicle metrics parquet file into a polars dataframe.\"\n",
    "    return scan_chronicle(path, \"metrics\", date, version = version) \n",
    "\n",
    "def scan_chronicle_logs(\n",
    "        path: str, # Path to dataset,\n",
    "        date:str = \"\", # date in format `YYYY/MM/DD` \n",
    "        version: str = \"v1\" # currently must be `v1`\n",
    ") -> pl.DataFrame:\n",
    "    \"Read a chronicle logs parquet file into a polars dataframe.\"\n",
    "    return scan_chronicle(path, \"logs\", date, version = version) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = scan_chronicle_metrics(\"./data\", \"2023/04/03\")\n",
    "assert type(z) == pl.LazyFrame\n",
    "assert z.collect().columns == [\n",
    "    'service',\n",
    "    'host',\n",
    "    'os',\n",
    "    'attributes',\n",
    "    'name',\n",
    "    'description',\n",
    "    'unit',\n",
    "    'type',\n",
    "    'timestamp',\n",
    "    'value_float',\n",
    "    'value_int',\n",
    "    'value_uint',\n",
    "    'value_column'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = scan_chronicle_logs(\"./data\", \"2023/04/03\")\n",
    "assert type(z) == pl.LazyFrame\n",
    "assert z.collect().columns == [\n",
    "    'service', \n",
    "    'host', \n",
    "    'os', \n",
    "    'attributes', \n",
    "    'body', \n",
    "    'timestamp'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# @pl.api.register_dataframe_namespace(\"metrics\")\n",
    "# class ChronicleMetrics:\n",
    "#     def __init__(self, \n",
    "#                  df: pl.DataFrame # A `polars` DataFrame\n",
    "#                  ) -> pl.DataFrame:\n",
    "#         \"Initialise a chronicle metrics class\"\n",
    "#         self._df = df\n",
    "\n",
    "@pl.api.register_lazyframe_namespace(\"metrics\")\n",
    "class ChronicleMetrics:\n",
    "    def __init__(self, \n",
    "                 ldf: pl.LazyFrame # A `polars` DataFrame\n",
    "                 ) -> pl.LazyFrame:\n",
    "        \"Initialise a chronicle metrics class\"\n",
    "        self._ldf = ldf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def describe(self: ChronicleMetrics) -> pl.DataFrame:\n",
    "    \"Reads metrics dataframe and returns a pandas dataframe with summary of service, name and description of all metrics\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .groupby(\"service\", \"name\")\n",
    "        .agg(\n",
    "            pl.col(\"description\").unique(),\n",
    "            pl.col(\"value_column\").unique(),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"description\").arr.join(\", \"),\n",
    "            pl.col(\"value_column\").arr.join(\"\")\n",
    "        )\n",
    "        .sort(\"service\", \"name\")\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scan_chronicle_metrics(\"../temp/\", \"2023/04/03\").metrics.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics data has a single row for each collected metric.\n",
    "\n",
    "Use `describe()` to get a DataFrame of the unique metrics in the metrics data, containing the `service`, `name` and `description` of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.describe()\n",
    "assert list(m) == ['service', 'name', 'description', 'value_column']\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter(self: ChronicleMetrics, \n",
    "        name:str, # name of metric to extract\n",
    "        alias:str = None # alias to use for new column\n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract a single metric from a metrics dataframe\"\n",
    "    if alias == None:\n",
    "        alias = name\n",
    "    return (\n",
    "        self._ldf\n",
    "        .filter(pl.col(\"name\") == name)\n",
    "        .sort(pl.col(\"host\"), pl.col(\"timestamp\"))\n",
    "        .select([\n",
    "            \"host\",\n",
    "            pl.col(\"timestamp\"),\n",
    "            pl.col(\"value_float\").alias(alias)\n",
    "        ])\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter the DataFrame on the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.filter(\"rsconnect_system_memory_used\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'rsconnect_system_memory_used']\n",
    "\n",
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\").metrics.filter(\"rsconnect_system_memory_used\", \"memory\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'memory']\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def plot(\n",
    "        self:ChronicleMetrics, # metrics dataframe\n",
    "        name:str, # name of metric to extract\n",
    "        alias:str = None # alias to use for new column\n",
    "    ) -> px.line: \n",
    "    \"Plot a selected metric using a Plotly line plot\"\n",
    "\n",
    "    dat = self._ldf.metrics.filter(name, alias)\n",
    "    fig = px.line(dat, x='timestamp', y=alias, line_group=\"host\", color=\"host\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = scan_chronicle_metrics(\"./data\", \"2023/04/03\")\n",
    "p = m.metrics.plot(\"rsconnect_system_memory_used\", \"memory\")\n",
    "assert str(type(p)) == \"<class 'plotly.graph_objs._figure.Figure'>\"\n",
    "\n",
    "p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@pl.api.register_lazyframe_namespace(\"logs\")\n",
    "class ChronicleLogs:\n",
    "    def __init__(self, \n",
    "                 df: pl.DataFrame # A polars data frame\n",
    "                 ) -> pl.DataFrame:\n",
    "        \"Initialise a chronicle logs DataFrame\"\n",
    "        self._ldf = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter logs on type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter_type(self: ChronicleLogs,\n",
    "                value: str # Value to extract \n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract all logs where type == value\"\n",
    "    return (\n",
    "        self._ldf\n",
    "        .with_columns([\n",
    "            (pl.col(\"body\").str.json_path_match(f\"$.{value}\").alias(f\".{value}\")),\n",
    "            (pl.col(\"body\").str.json_path_match(\"$.type\").alias(\".type\"))\n",
    "        ])\n",
    "        .filter(pl.col(f\".{value}\").is_not_null())\n",
    "        .select([\"service\", \"host\", \"timestamp\", f\".{value}\", \".type\", \"body\"])\n",
    "        .sort(\"service\", \"host\", f\".{value}\", \"timestamp\")\n",
    "        .collect()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = scan_chronicle_logs(\"./data\", \"2023/04/03\").logs.filter_type(\"username\")\n",
    "assert type(logs) == pl.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_chronicle_logs(\"./data\", \"2023/04/03\").logs.filter_type(\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
