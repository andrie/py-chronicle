{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Read and query chronicle parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "from s3fs import S3FileSystem\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read chronicle parquet files\n",
    "\n",
    "Chronicle collects and stores logs and metrics in a series of parquet files.\n",
    "\n",
    "Use `read_chronicle()` to read either logs or metrics, by specifying the path to the parquet set you need.\n",
    "\n",
    "The file tree looks like this, with `logs` and `metrics` in separate folders inside `v1`.\n",
    "\n",
    "``` bash\n",
    ".\n",
    "└── v1/\n",
    "    ├── logs/\n",
    "    └── metrics/\n",
    "```\n",
    "\n",
    "Inside both `logs` and `metrics` the data is stored by date, separated by year, month and day.\n",
    "\n",
    "``` bash\n",
    ".\n",
    "└── v1/\n",
    "    ├── logs/\n",
    "    │   └── 2023/\n",
    "    │       ├── 02/\n",
    "    │       │   ├── 01\n",
    "    │       │   ├── 02\n",
    "    │       │   ├── 03\n",
    "    │       │   ├── 04\n",
    "    │       │   ├── 05\n",
    "    │       │   └── ...\n",
    "    │       ├── 03\n",
    "    │       ├── 04\n",
    "    │       └── ...\n",
    "    └── metrics/\n",
    "        └── 2023/\n",
    "            ├── 02/\n",
    "            │   ├── 01\n",
    "            │   ├── 02\n",
    "            │   ├── 03\n",
    "            │   ├── 04\n",
    "            │   ├── 05\n",
    "            │   └── ...\n",
    "            ├── 03\n",
    "            ├── 04\n",
    "            └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_chronicle(\n",
    "        path: str # Path to dataset\n",
    "    ) -> pl.DataFrame:\n",
    "    \"Read a chronicle parquet file into a polars dataframe.\"\n",
    "    return pl.from_arrow(pq.read_table(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = read_chronicle(\"./data/v1/metrics\")\n",
    "assert type(z) == pl.dataframe.frame.DataFrame\n",
    "\n",
    "z = read_chronicle(\"./data/v1/logs\")\n",
    "assert type(z) == pl.dataframe.frame.DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@pl.api.register_dataframe_namespace(\"metrics\")\n",
    "class ChronicleMetrics:\n",
    "    def __init__(self, \n",
    "                 df: pl.DataFrame # A `polars` DataFrame\n",
    "                 ) -> pl.DataFrame:\n",
    "        \"Initialise a chronicle metrics class\"\n",
    "        self._df = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def describe(self: ChronicleMetrics) -> pl.DataFrame:\n",
    "    \"Reads metrics dataframe and returns a pandas dataframe with summary of service, name and description of all metrics\"\n",
    "    return (\n",
    "        self._df\n",
    "        .groupby(\"service\", \"name\")\n",
    "        .agg(\n",
    "            pl.col(\"description\").unique(),\n",
    "            pl.col(\"value_column\").unique(),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"description\").arr.join(\", \"),\n",
    "            pl.col(\"value_column\").arr.join(\"\")\n",
    "        )\n",
    "        .sort(\"service\", \"name\")\n",
    "        .to_pandas()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics data has a single row for each collected metric.\n",
    "\n",
    "Use `describe()` to get a DataFrame of the unique metrics in the metrics data, containing the `service`, `name` and `description` of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = read_chronicle(\"./data/v1/metrics/\").metrics.describe()\n",
    "assert list(m) == ['service', 'name', 'description', 'value_column']\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter(self: ChronicleMetrics, \n",
    "        name:str, # name of metric to extract\n",
    "        alias:str = None # alias to use for new column\n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract a single metric from a metrics dataframe\"\n",
    "    if alias == None:\n",
    "        alias = name\n",
    "    return (\n",
    "        self._df\n",
    "        .lazy()\n",
    "        .filter(pl.col(\"name\") == name)\n",
    "        .sort(pl.col(\"host\"), pl.col(\"timestamp\"))\n",
    "        .select([\n",
    "            \"host\",\n",
    "            pl.col(\"timestamp\"),\n",
    "            pl.col(\"value_float\").alias(alias)\n",
    "        ])\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `name` argument is used to filter the DataFrame on the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = read_chronicle(\"./data/v1/metrics/\").metrics.filter(\"rsconnect_system_memory_used\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'rsconnect_system_memory_used']\n",
    "\n",
    "m = read_chronicle(\"./data/v1/metrics/\").metrics.filter(\"rsconnect_system_memory_used\", \"memory\")\n",
    "assert type(m) == pd.DataFrame\n",
    "assert list(m) == ['host', 'timestamp', 'memory']\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def plot(\n",
    "        self:ChronicleMetrics, # metrics dataframe\n",
    "        name:str, # name of metric to extract\n",
    "        alias:str = None # alias to use for new column\n",
    "    ) -> px.line: \n",
    "    \"Plot a selected metric using a Plotly line plot\"\n",
    "\n",
    "    dat = self._df.metrics.filter(name, alias) \n",
    "    fig = px.line(dat, x='timestamp', y=alias, line_group=\"host\", color=\"host\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = read_chronicle(\"./data/v1/metrics/\")\n",
    "p = m.metrics.plot(\"rsconnect_system_memory_used\", \"memory\")\n",
    "assert str(type(p)) == \"<class 'plotly.graph_objs._figure.Figure'>\"\n",
    "\n",
    "\n",
    "read_chronicle(\"./data/v1/metrics/\").metrics.plot(\"rsconnect_system_memory_used\", \"memory\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@pl.api.register_dataframe_namespace(\"logs\")\n",
    "class ChronicleLogs:\n",
    "    def __init__(self, \n",
    "                 df: pl.DataFrame # A polars data frame\n",
    "                 ) -> pl.DataFrame:\n",
    "        \"Initialise a chronicle logs DataFrame\"\n",
    "        self._df = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter logs on type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter_type(self: ChronicleLogs,\n",
    "                value: str # Value to extract \n",
    "    ) -> pd.DataFrame:\n",
    "    \"Extract all logs where type == value\"\n",
    "    return (\n",
    "        self._df\n",
    "        .lazy()\n",
    "        .with_columns([\n",
    "            (pl.col(\"body\").str.json_path_match(f\"$.{value}\").alias(f\".{value}\")),\n",
    "            (pl.col(\"body\").str.json_path_match(\"$.type\").alias(\".type\"))\n",
    "        ])\n",
    "        .filter(pl.col(f\".{value}\").is_not_null())\n",
    "        .select([\"service\", \"host\", \"timestamp\", f\".{value}\", \".type\", \"body\"])\n",
    "        .sort(\"service\", \"host\", f\".{value}\", \"timestamp\")\n",
    "        .collect()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = read_chronicle(\"./data/v1/logs\").logs.filter_type(\"username\")\n",
    "assert type(logs) == pl.DataFrame\n",
    "\n",
    "# assert logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "read_chronicle(\"./data/v1/logs\").logs.filter_type(\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
