{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# io\n",
    "\n",
    "> File operations on chronicle parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import fs\n",
    "# import pyarrow.dataset as ds\n",
    "# from s3fs import S3FileSystem\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "from chronicle.core import read_chronicle_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_parquet(\n",
    "        x: pl.DataFrame, # polars DataFrame\n",
    "        filename:str # Full file file name\n",
    "    ) -> None:\n",
    "    \"Write chronicle data to parquet file\"\n",
    "    return pq.write_table(x.to_arrow(), filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = read_chronicle_metrics(\"./data\")\n",
    "\n",
    "# create a temporary file\n",
    "tf = tempfile.NamedTemporaryFile(suffix = \".parquet\")\n",
    "assert os.path.getsize(tf.name) == 0\n",
    "z = write_parquet(m, tf)\n",
    "\n",
    "assert os.path.getsize(tf.name) > 0\n",
    "assert z is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_s3_bucket_dates(\n",
    "        bucket:str, # S3 bucket name, without the \"s3://\" prefix \n",
    "        type=\"logs\", # \"logs\" or \"metrics\"\n",
    "        version=\"v1\" # \"v1\" or \"v2\"\n",
    "    ) -> list:\n",
    "    \"Get a list of dates for which there are chronicle logs or metrics in an S3 bucket\"\n",
    "    s3 = fs.S3FileSystem()\n",
    "    p = s3.get_file_info(\n",
    "        fs.FileSelector(\n",
    "            f'{bucket}/{version}/{type}', \n",
    "            recursive=True)\n",
    "        )\n",
    "    # list all paths where type == file\n",
    "    ps = [x.path for x in p if x.type == 2]\n",
    "    # extract dates using a regular expression\n",
    "    dates = [re.findall(r'\\d{4}/\\d{2}/\\d{2}', x)[0] for x in ps]\n",
    "    # convert to a set to get unique values\n",
    "    dates = list(set(dates))\n",
    "    dates.sort()\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "bucket = \"colorado-posit-chronicle\"\n",
    "get_s3_bucket_dates(bucket, \"metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
